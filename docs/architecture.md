# ğŸ§  KubeNap Architecture

KubeNap is a Kubernetes-native controller that automatically suspends and resumes deployments based on real-time HTTP activity, without introducing latency during normal operation. This document outlines the architectural design, data flow, and integration touchpoints.

---

## ğŸ¯ Design Goals

* Avoid inline proxies or latency during normal operation
* Use only standard Kubernetes APIs (Service, EndpointSlice, Deployment)
* Support multiple reverse proxies (Traefik, NGINX, Istio, etc.) via plugin integration
* Provide clean wake-resume-redirection cycle
* Be opt-in via annotations

---

## ğŸ“ Key Components

| Component          | Description                                                                    |
| ------------------ | ------------------------------------------------------------------------------ |
| Deployment Monitor | Watches annotated deployments, determines idleness via pluggable metric source |
| Wake Handler       | HTTP handler for resume-on-request functionality                               |
| Proxy Plugin       | Optional proxy-specific code to forward traffic to KubeNap when app is asleep  |
| Metrics Source     | Pluggable interface to query traffic data (Prometheus, InfluxDB, etc.)         |

---

## ğŸ” Lifecycle Flow (Text-Based)

```text
1. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Ingress   â”‚  â†â”€ receives user HTTP request
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
2. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Kubernetes     â”‚
   â”‚ Service        â”‚  â†’ attempts to route to app pods
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
3. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ No Active Endpointsâ”‚  â†’ app is scaled to 0 replicas
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
4. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Ingress returns 502 or triggers wake path â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
5. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ KubeNap receives wake call â”‚ â† via special path or 502 handler
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
6. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ KubeNap looks up associated          â”‚
   â”‚ Deployment via annotations           â”‚
   â”‚ (e.g. kubenap/service) â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
7. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ KubeNap scales replicas > 0   â”‚
   â”‚ (from stored replicaCount)    â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
8. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Wait for Deployment Ready  â”‚ â† poll Pod readiness or use informers
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
9. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ KubeNap sends 307 Redirect   â”‚
   â”‚ to original request URL      â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
10.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Client reissues     â”‚ â† browser/curl/client follows redirect
   â”‚ request to Ingress  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
11.â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Ingress â†’ App Service â†’ Podâ”‚
   â”‚ (now app has endpoints)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Reverse Proxy Integration

KubeNap uses a minimal plugin interface per ingress type:

### Traefik:

* Use `errorpage` middleware to rewrite 502 to KubeNap's `/wake` endpoint

### NGINX:

* Use `error_page 502 = @handler` to invoke KubeNap handler via internal redirect

### Istio/Envoy:

* Use WASM or Lua filter to intercept failure response and retry via `/wake`

---

## ğŸ“Œ Annotations

```yaml
metadata:
  labels:
    kubenap/enabled: "true"
  annotations:
    kubenap/service: "myapp-svc"
    kubenap/ingress: "myapp-ing"
    kubenap/idleAfter: "10m"
    kubenap/replicaCount: "1"
    kubenap/proxy: "nginx" # or traefik, istio
```

---

## ğŸ“ˆ Metrics Design

KubeNap supports plug-and-play metric backends:

| Backend    | Method                             |
| ---------- | ---------------------------------- |
| Prometheus | Query `http_requests_total`        |
| InfluxDB   | Query Flux expressions             |
| Datadog    | REST API to check app traffic      |
| Logs       | Tail NGINX/Envoy logs if necessary |

Backends implement a `TrafficMetricsSource` interface returning the last-seen activity timestamp.

---

## âœ… Summary

* âš¡ KubeNap does not sit inline with traffic â€” zero latency overhead
* ğŸ§  Relies on annotations to opt-in and configure behavior
* ğŸ“Š Traffic-aware using pluggable metric sources
* ğŸ” 307-based redirect ensures seamless client experience
* ğŸ”Œ Designed for pluggable integration with any ingress
